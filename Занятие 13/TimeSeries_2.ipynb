{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Временные ряды, часть 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('hour_online.csv', index_col=['Time'], parse_dates=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(dataset['Users'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скользящее среднее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Напишите функцию, вычисляющую скользящее среднее, а именно, по данному ряду series и числу n вычислите среднее значение последних n значений ряда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(series, n):\n",
    "    #your code here\n",
    "    \n",
    "moving_average(dataset.Users, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMovingAverage(series, n):\n",
    "\n",
    "    \"\"\"\n",
    "    series - dataframe with timeseries\n",
    "    n - rolling window size \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rolling_mean = series.rolling(window=n).mean()\n",
    "\n",
    "    # При желании, можно строить и доверительные интервалы для сглаженных значений\n",
    "    #rolling_std =  series.rolling(window=n).std()\n",
    "    #upper_bond = rolling_mean+1.96*rolling_std\n",
    "    #lower_bond = rolling_mean-1.96*rolling_std\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"Moving average\\n window size = {}\".format(n))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    #plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n",
    "    #plt.plot(lower_bond, \"r--\")\n",
    "    plt.plot(dataset[n:], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(dataset, 24) # сглаживаем по дням\n",
    "plotMovingAverage(dataset, 24*7) # сглаживаем по неделям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Взвешенное среднее\n",
    "\n",
    "### Задание 2. Напишите функцию, вычисляющую взвешенное среднее, а именно, вычисляющую сумму последних значений ряда series с весами, лежащими в списке weights.\n",
    "\n",
    "$$weights[0] \\cdot series[-1] + weights[1] \\cdot series[-2] + ... + weights[n] \\cdot series[-n-1]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(series, weights):\n",
    "    #your code here\n",
    "    \n",
    "weighted_average(dataset.Users, [0.6, 0.2, 0.1, 0.07, 0.03])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Экспоненциальное сглаживание\n",
    "\n",
    "### Задание 3. Напишите функцию для экспоненциального сглаживания, которая каждое значение ряда преобразует по формуле:\n",
    "$$result[n] = alpha \\cdot series[n] + (1 - alpha) \\cdot result[n-1]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        #your code here\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for alpha in [0.3, 0.05]:\n",
    "        plt.plot(exponential_smoothing(dataset.Users, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(dataset.Users.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Двойное экспоненциальное сглаживание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # прогнозируем\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "        \n",
    "#        print('level=',level,' trend=',trend)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for alpha in [0.9, 0.02]:\n",
    "        for beta in [0.9, 0.02]:\n",
    "            plt.plot(double_exponential_smoothing(dataset.Users, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.plot(dataset.Users.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Double Exponential Smoothing\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тройное экспоненциальное сглаживание (модель Хольта-Винтерса)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "\n",
    "    \"\"\"\n",
    "    Модель Хольта-Винтерса с методом Брутлага для детектирования аномалий\n",
    "    https://fedcsis.org/proceedings/2012/pliks/118.pdf\n",
    "\n",
    "    # series - исходный временной ряд\n",
    "    # slen - длина сезона\n",
    "    # alpha, beta, gamma - коэффициенты модели Хольта-Винтерса\n",
    "    # n_preds - горизонт предсказаний\n",
    "    # scaling_factor - задаёт ширину доверительного интервала по Брутлагу (обычно принимает значения от 2 до 3)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "\n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # вычисляем сезонные средние\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # вычисляем начальные значения\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "\n",
    "        seasonals = self.initial_seasonal_components()\n",
    "\n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # инициализируем значения компонент\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "\n",
    "                self.PredictedDeviation.append(0)\n",
    "\n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "\n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "\n",
    "                continue\n",
    "            if i >= len(self.series): # прогнозируем\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "\n",
    "                # во время прогноза с каждым шагом увеличиваем неопределенность\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "\n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "\n",
    "                # Отклонение рассчитывается в соответствии с алгоритмом Брутлага\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "\n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i % self.slen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for alpha in [0.9, 0.02]:\n",
    "        for beta in [0.9, 0.02]:\n",
    "                for gamma in [0.05]:\n",
    "                    HW = HoltWinters(dataset.Users, 3, alpha, beta, gamma, 1)\n",
    "                    HW.triple_exponential_smoothing()\n",
    "                    plt.plot(HW.result, label=\"Alpha {}, beta {}, gamma {}\".format(alpha, beta, gamma))\n",
    "    plt.plot(dataset.Users.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Triple Exponential Smoothing\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кросс-валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Разбейте временной ряд series на две части, используя функцию TimeSeriesSplit с количеством фолдов n_splits=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseriesCVscore(x):\n",
    "    # вектор ошибок\n",
    "    errors = []\n",
    "\n",
    "    values = data.values\n",
    "    alpha, beta, gamma = x\n",
    "\n",
    "    # задаём число фолдов для кросс-валидации\n",
    "    tscv = TimeSeriesSplit(n_splits=3) \n",
    "    \n",
    "    # идем по фолдам, на каждом обучаем модель, строим прогноз на отложенной выборке и считаем ошибку\n",
    "    for train, test in tscv.split(values):\n",
    "\n",
    "        model = HoltWinters(series=values[train], slen = 24*7, alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n",
    "        model.triple_exponential_smoothing()\n",
    "\n",
    "        predictions = model.result[-len(test):]\n",
    "        actual = values[test]\n",
    "        error = mean_squared_error(predictions, actual)\n",
    "        errors.append(error)\n",
    "\n",
    "    # Возвращаем средний квадрат ошибки по вектору ошибок \n",
    "    return np.mean(np.array(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = dataset.Users[:-500] # отложим часть данных для тестирования\n",
    "\n",
    "# инициализируем значения параметров\n",
    "x = [0, 0, 0] \n",
    "\n",
    "# Минимизируем функцию потерь с ограничениями на параметры\n",
    "opt = minimize(timeseriesCVscore, x0=x, method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1)))\n",
    "\n",
    "# Из оптимизатора берем оптимальное значение параметров\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.Users\n",
    "model = HoltWinters(data[:-128], slen = 24*7, alpha = alpha_final, beta = beta_final, gamma = gamma_final, n_preds = 128, scaling_factor = 2.56)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHoltWinters():\n",
    "    Anomalies = np.array([np.NaN]*len(data))\n",
    "    Anomalies[data.values<model.LowerBond] = data.values[data.values<model.LowerBond]\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(model.result, label = \"Model\")\n",
    "    plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n",
    "    plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n",
    "    plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, y2=model.LowerBond, alpha=0.5, color = \"grey\")\n",
    "    plt.plot(data.values, label = \"Actual\")\n",
    "    plt.plot(Anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    plt.axvspan(len(data)-128, len(data), alpha=0.5, color='lightgrey')\n",
    "    plt.grid(True)\n",
    "    plt.axis('tight')\n",
    "    plt.legend(loc=\"best\", fontsize=13);\n",
    "\n",
    "plotHoltWinters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение. \n",
    "\n",
    "## Извлечение признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Возвращает словарь, где ключами являются уникальные категории признака cat_feature, \n",
    "    а значениями - средние по real_feature\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(dataset)\n",
    "data.columns = [\"y\"]\n",
    "\n",
    "#data.index = data.index.to_datetime()\n",
    "data[\"hour\"] = data.index.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5. Создайте столбец weekda в таблице data, куда запишите день недели, взятый из даты, находящейся в data.index. После этого создайте столбец is_weekend, в который запишите 1, если день недели выходной, и 0 - иначе. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mean(data, 'weekday', \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень хорошими признаками для решения задач, связанных с временными рядами с помощью машинного обучения, являются лаги. Лаг - это ряд, сдвинутый на несколько моментов времени назад."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.y.shift(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6. Выведите на экран (с помощью цикла) последние 10 значений первых 10 лагов ряда: сначала 10 значений самого ряда, затем 10 значений ряда, свдинутого на 1, затем 10 значений ряда, сдвинутого на 2 и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для извлечения всех рассмотренных выше признаков (средние значения, признаки по дате, лаговые признаки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(data, lag_start=5, lag_end=20, test_size=0.15):\n",
    "\n",
    "    data = pd.DataFrame(data.copy())\n",
    "    data.columns = [\"y\"]\n",
    "\n",
    "    # считаем индекс в датафрейме, после которого начинается тестовый отрезок\n",
    "    test_index = int(len(data)*(1-test_size))\n",
    "\n",
    "    # добавляем лаги исходного ряда в качестве признаков\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "\n",
    "#    data.index = data.index.to_datetime()\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "\n",
    "    # считаем средние только по тренировочной части, чтобы избежать лика\n",
    "\n",
    "    data['weekday_average'] = list(map(code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday))\n",
    "    data[\"hour_average\"] = list(map(code_mean(data[:test_index], 'hour', \"y\").get, data.hour))\n",
    "    # выкидываем закодированные средними признаки \n",
    "    data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # разбиваем весь датасет на тренировочную и тестовую выборку\n",
    "    X_train = data.loc[:test_index].drop([\"y\"], axis=1)\n",
    "    y_train = data.loc[:test_index][\"y\"]\n",
    "    X_test = data.loc[test_index:].drop([\"y\"], axis=1)\n",
    "    y_test = data.loc[test_index:][\"y\"]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepareData(dataset.Users, test_size=0.3, lag_start=1, lag_end=48)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "prediction = lr.predict(X_test)\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(prediction, \"r\", label=\"prediction\")\n",
    "plt.plot(y_test.values, label=\"actual\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Linear regression\\n Mean absolute error {} users\".format(round(mean_absolute_error(prediction, y_test))))\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performTimeSeriesCV(X_train, y_train, number_folds, model, metrics):\n",
    "    print('Size train set: {}'.format(X_train.shape))\n",
    "\n",
    "    k = int(np.floor(float(X_train.shape[0]) / number_folds))\n",
    "    print('Size of each fold: {}'.format(k))\n",
    "\n",
    "    errors = np.zeros(number_folds-1)\n",
    "\n",
    "    # loop from the first 2 folds to the total number of folds    \n",
    "    for i in range(2, number_folds + 1):\n",
    "        print('')\n",
    "        split = float(i-1)/i\n",
    "        print('Splitting the first ' + str(i) + ' chunks at ' + str(i-1) + '/' + str(i) )\n",
    "\n",
    "        X = X_train[:(k*i)]\n",
    "        y = y_train[:(k*i)]\n",
    "        print('Size of train + test: {}'.format(X.shape)) # the size of the dataframe is going to be k*i\n",
    "\n",
    "        index = int(np.floor(X.shape[0] * split))\n",
    "\n",
    "        # folds used to train the model        \n",
    "        X_trainFolds = X[:index]        \n",
    "        y_trainFolds = y[:index]\n",
    "\n",
    "        # fold used to test the model\n",
    "        X_testFold = X[(index + 1):]\n",
    "        y_testFold = y[(index + 1):]\n",
    "\n",
    "        model.fit(X_trainFolds, y_trainFolds)\n",
    "        errors[i-2] = metrics(model.predict(X_testFold), y_testFold)\n",
    "\n",
    "    # the function returns the mean of the errors on the n-1 folds    \n",
    "    return errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "performTimeSeriesCV(X_train, y_train, 5, lr, mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def RandomForest_forecast(data, lag_start=5, lag_end=20, test_size=0.3, scale=1.96):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = prepareData(dataset.Users, lag_start, lag_end, test_size)\n",
    "\n",
    "    RF = RandomForestRegressor(n_estimators=100)\n",
    "    RF.fit(X_train, y_train)\n",
    "    \n",
    "    # посмотрим, как модель вела себя на тренировочном отрезке ряда\n",
    "    prediction_train = RF.predict(X_train)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(prediction_train)\n",
    "    plt.plot(y_train)\n",
    "    plt.axis('tight')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # и на тестовом\n",
    "    prediction_test = RF.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(prediction_test, label=\"prediction\")\n",
    "    plt.plot(list(y_test), label=\"y_test\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"RandomForest Mean absolute error {} users\".format(round(mean_absolute_error(prediction_test, y_test))))\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_forecast(dataset, test_size=0.3, lag_start=1, lag_end=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('hour_online.csv', index_col=['Time'], parse_dates=['Time'])\n",
    "d = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.rename(columns = {'Time':'ds', 'Users':'y'}, inplace = True)\n",
    "print(d.head())\n",
    "\n",
    "train_dataset = d.iloc[:-800]\n",
    "test_dataset = d.iloc[-800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet() #daily_seasonality, weekly_seasonality\n",
    "m.fit(d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(freq='H',periods=300) #freq='H' означает, что измерения проводятся каждый час\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = forecast['yhat'][-800:]\n",
    "actual = d_test['y']\n",
    "mean_absolute_error(prediction,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plot(forecast['yhat'][-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = forecast.set_index('ds')[['yhat']].join(d.set_index('ds').y).reset_index()\n",
    "metric_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(metric_df.yhat, label=\"prediction\")\n",
    "plt.plot(list(metric_df.y), label=\"y_test\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.axis('tight')\n",
    "plt.title(\"FBProphet Mean absolute error {} users\".format(round(mean_absolute_error(metric_df.yhat, metric_df.y))))\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные по временным рядам \n",
    "\n",
    "https://data.world/datasets/time-series\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=10to100&numIns=&type=ts&sort=nameUp&view=table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "Скачайте с одного из предложенных сайтов (или с любого другого) наиболее понравившийся вам датасет и далее работайте с ним.\n",
    "\n",
    "Разбейте данные на train и test (test - последние 15%-30% данных в зависимости от размера датасета).\n",
    "\n",
    "1) Постройте адаптивные модели двойного экспоненциального сглаживания и тройного экспоненциального сглаживания. \n",
    "Сделайте предсказания и измерьте ошибки mse и mae на train и test.\n",
    "\n",
    "2*) Исследуйте данные на стационарность. Проверьте наличие сезонных эффектов, зависимостей от предыдущих значений ряда, сбалансируйте дисперсию с помощью преобразования Бокса-Кокса. Приведите ряд к стационарному и примените к полученному стационарному ряду модель ARMA. Подберите параметры p и q для ARMA.\n",
    "\n",
    "3) Постройте предсказание с помощью модели ARIMA (или SARIMA, если вы подозреваете наличие сезонных эффектов в данных). Измерьте ошибки предсказаний на train и test.\n",
    "\n",
    "4) Сделайте предсказание с помощью fbprophet и измерьте mse и mae предсказания на train и на test.\n",
    "\n",
    "5) Поработайте над созданием новых признаков (лаги, аггрегированные признаки и т.д.) и примените для предсказания линейную регрессию и какой-нибудь нелинейный алгоритм.\n",
    "\n",
    "6) Сравните результаты всех полученных алгоритмов. Какой алгоритм дал наилучшее качество? Какой алгоритм имеет самое маленькое время настройки параметров при хорошем качестве?\n",
    "\n",
    "7*) Попробуйте скомбинировать наилучшие модели. Улучшилось ли качество предсказания?\n",
    "\n",
    "Задания 2 и 7 опциональные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
