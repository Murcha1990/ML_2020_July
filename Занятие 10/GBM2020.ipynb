{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг\n",
    "## 1. Сравнение градиентного бустинга и решающего леса.\n",
    "\n",
    "Сравним, как ведут себя бустинг и бэггинг с ростом числа базовых алгоритмов.\n",
    "\n",
    "В случае бэггинга все базовые алгоритмы настраиваются на различные выборки из одного и того же распределения. При этом некоторые из них могут оказаться переобученными, однако усреднение позволяет ослабить этот эффект (объясняется тем, что для некоррелированных алгоритмов разброс композиции оказывается в $N$ раз меньше разброса отдельных алгоритмов, т.е. много деревьев с меньшей вероятностью настроятся на некоторый нетипичный объект по сравнению с одним деревом). Если $N$ достаточно велико, то последующие добавления новых алгоритмов уже не позволят улучшить качество модели.\n",
    "\n",
    "В случае же бустинга каждый алгоритм настраивается на ошибки всех предыдущих, это позволяет на каждом шаге настраиваться на исходное распределение все точнее и точнее. Однако при достаточно большом $N$ это является источником переобучения, поскольку последующие добавления новых алгоритмов будут продолжать настраиваться на обучающую выборку, уменьшая ошибку на ней, при этом уменьшая обобщающую способность итоговой композиции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- сгенерируем точки, разбитые на две группы: внутри каждой группы точки лежат в окрестности некоторой прямой\n",
    "- наша цель: понять, смогут ли бэггинг и градиентный бустинг понять эту зависимость (т.е. понять, что точки внутри каждйо группы лежат в окрестности некоторой прямой)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.linspace(0, 1, 100)\n",
    "X_test = np.linspace(0, 1, 1000)\n",
    "\n",
    "def target(x):\n",
    "    return x > 0.5\n",
    "\n",
    "Y_train = target(X_train) + np.random.randn(*X_train.shape) * 0.1\n",
    "\n",
    "plt.scatter(X_train, Y_train, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с бэггинга, а именно, с решающего леса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "reg = RandomForestRegressor(max_depth=2)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in tqdm(enumerate(sizes)):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что с некоторого момента итоговая функция перестает меняться с ростом количества деревьев.\n",
    "\n",
    "Теперь проделаем то же самое для градинентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=1)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in tqdm(enumerate(sizes)):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный бустинг довольно быстро построил истинную зависимость, после чего начал настраиваться уже на конкретные объекты обучающей выборки, из-за чего сильно переобучился.\n",
    "\n",
    "\n",
    "Бороться с этой проблемой можно с помощью выбора очень простого базового алгоритма или\n",
    "же искусственным снижением веса новых алгоритмов при помощи ***шага $\\eta$***:\n",
    "$$a_N(x) = \\sum_{n=0}^N \\eta \\gamma_N b_n(x).$$\n",
    "\n",
    "***Такая поправка замедляет обучение по сравнению с бэггингом, но зато позволяет получить менее переобученный алгоритм***. Тем не менее, важно понимать, что переобучение всё равно будет иметь место при обучении сколь угодно большого количества базовых алгоритмов для фиксированного $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=0.1, warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in tqdm(enumerate(sizes)):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Решение задачи регрессии с помощью градиентного бустинга и решающего леса.\n",
    "\n",
    "Будем решать задачу регрессии на примере датасета **Diabetes**.\n",
    "По различным факторам будем предсказывать количественный эффект от лечения диабетиков спустя год."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = datasets.load_diabetes()\n",
    "print(ds.DESCR)\n",
    "X = ds.data\n",
    "Y = ds.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание.\n",
    "\n",
    "* Разбейте данные на train и test. \n",
    "* Объявите модель бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = #your code here\n",
    "\n",
    "MAX_ESTIMATORS = 250\n",
    "\n",
    "model = #your code here\n",
    "\n",
    "err_train_gb = []\n",
    "err_test_gb = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого числа деревьев в промежутке (1, MAX_ESTIMATORS+1) обучим на train градиентный бустинг и запишем в созданные списки полученную ошибку: 1 - r2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    model.n_estimators = i\n",
    "    model.fit(X_train, Y_train)\n",
    "    err_train_gb.append(1 - model.score(X_train, Y_train))\n",
    "    err_test_gb.append(1 - model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторите процедуру для решающего леса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=222)\n",
    "err_train_bag = []\n",
    "err_test_bag = []\n",
    "\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    #your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(err_train_gb, label='GB')\n",
    "plt.plot(err_train_bag, label='RF')\n",
    "plt.legend()\n",
    "plt.title('Train')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(err_test_gb, label='GB')\n",
    "plt.plot(err_test_bag, label='RF')\n",
    "plt.legend()\n",
    "plt.title('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Мы видим, что случайный лес достигает некоторой минимальной ошибки на обучении и на тесте и дальше не меняет своего качества.\n",
    "\n",
    "* При этом ошибка градиентного бустинга всё время уменьшается на обучении (стремится к нулю), а на тесте с какого-то момента начинает расти, то есть начинается переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ответим на следующие вопросы:\n",
    "* ***Как подобрать количество деревьев в RandomForest для достижения наилучшего качества и наименьших временных затрат?***\n",
    "* ***Как подобрать количество деревьев в GradientBoosting для достижения наилучшего качества и не переобучиться?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ответим на первый вопрос**: как подобрать количество деревьев в RandomForest для достижения наилучшего качества и наименьших временных затрат? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_train_bag = np.array(err_train_bag)\n",
    "err_test_bag = np.array(err_test_bag)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(err_train_bag, label='RF')\n",
    "plt.plot([0, err_train_bag.shape[0]], [err_train_bag.min(), err_train_bag.min()], 'g--')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Train')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(err_test_bag, label='RF')\n",
    "plt.plot([0, err_test_bag.shape[0]], [err_test_bag.min(), err_test_bag.min()], 'g--')\n",
    "plt.legend()\n",
    "plt.title('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что начиная примерно с 50-60 деревьев в лесе ошибка на train и на test перестает уменьшаться и колеблется около своего минимума. Поэтому ***для обучения леса в этой задаче достаточно 50-60 деревьев***.\n",
    "\n",
    "**Ответим на второй вопрос**: как подобрать количество деревьев в GradientBoosting для достижения наилучшего качества и не переобучиться? \n",
    "\n",
    "Так как *ошибка на трейне в бустинге всегда уменьшается*, то ***для снижения переобучения найдем количество деревьев, требуемое для достижения минимальной ошибки на тесте***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_train_gb = np.array(err_train_gb)\n",
    "err_test_gb = np.array(err_test_gb)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(err_test_gb, label='GB')\n",
    "plt.legend()\n",
    "plt.title('Test')\n",
    "\n",
    "plt.plot([0, err_test_gb.shape[0]], [err_test_gb.min(), err_test_gb.min()], 'g--')\n",
    "plt.plot([err_test_gb.argmin()], [err_test_gb.min()], 'v')\n",
    "plt.title('test error=%.3f, best_est=%d' % (err_test_gb.min(), err_test_gb.argmin()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что минимальная ошибка достигается при $n=25$ деревьях.\n",
    "\n",
    "Сравним ошибку решающего леса и ошибку бустинга на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest:', err_test_bag.min(), 'n_trees:', err_test_bag.argmin()+1)\n",
    "print('Gradient Boosting:', err_test_gb.min(), 'n_trees:', err_test_gb.argmin()+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы:\n",
    "\n",
    "* Мы видим, что в данной задаче случайный лес дает качество, немного лучшее, чем градиентный бустинг. В других задачах может получиться обратная ситуация.\n",
    "\n",
    "* Случайный лес не переобучается, поэтому для него не обязательно искать оптимальное количество деревьев (полезно только найти минимальное необходимое число деревьев, чтобы снизить время обучения).\n",
    "\n",
    "* Для градиентного бустинга ***необходимо*** находить оптимальное количество деревьев, чтобы избежать переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, можно ли улучшить качество градиентного бустинга за счёт подбора гиперпараметров.\n",
    "\n",
    "Без подбора параметров с оптимальным числом деревьев градиентный бустинг дает следующее качество:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=25, random_state=111)\n",
    "\n",
    "cross_val_score(model, X, Y, cv=3, scoring='r2').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем основные параметры (n_estimators, max_features, max_depth) по сетке. Так как поиск по сетке занимает длительное время, число деревьев будем искать в маленьком диапазоне и с шагом 5.\n",
    "\n",
    "![Поиск по сетке (GridSearch)](gridsearch.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%time\n",
    "gs = GridSearchCV(GradientBoostingRegressor(random_state=111),\n",
    "                  param_grid={'n_estimators': range(10,50,5),\n",
    "                             'max_features': range(1,X.shape[1]+1), \n",
    "                             'max_depth': range(2,20)},\n",
    "                  cv=3,\n",
    "                  scoring='r2', verbose=1, n_jobs=-1)\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_, gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устроим более полный поиск параметров по сетке: будем также искать learning_rate, min_samples_leaf, min_samples_split, subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "gs = GridSearchCV(GradientBoostingRegressor(random_state=111),\n",
    "                  param_grid={'n_estimators': range(10,500),\n",
    "                             'max_features': range(1,X.shape[1]+1), \n",
    "                             'max_depth': range(2,10),\n",
    "                             'learning_rate': np.arange(0.1,1.1,0.1),\n",
    "                             'min_samples_leaf': range(1,10),\n",
    "                             'min_samples_split': np.arange(0.1,1.1,0.1),\n",
    "                             'subsample': np.arange(0.1,1.1,0.1)},\n",
    "                  cv=3,\n",
    "                  scoring='r2', verbose=1, n_jobs=-1)\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что поиск по большому числу параметров занимает очень много времени. Поэтому используем другой вариант подбора параметров, основанный на байесовских методах - **hyperopt**.\n",
    "\n",
    "Для того, чтобы уменьшить число итераций до нахождения хорошей конфигурации, придуманы адаптивные байесовские методы. Они выбирают следующую точку для проверки, учитывая результаты на уже проверенных точках. Идея состоит в том, чтобы на каждом шаге найти компромисс между (а) исследованием регионов рядом с самыми удачными точками среди найденных и (б) исследованием регионов с большой неопределенностью, где могут находиться еще более удачные точки. Это часто называют дилеммой explore-exploit или «learning vs earning». Таким образом, в ситуациях, когда проверка каждой новой точки стоит дорого (в машинном обучении проверка = обучение + валидация), можно приблизиться к глобальному оптимуму за гораздо меньшее число шагов (подробнее см. https://habr.com/ru/company/dca/blog/272697/).\n",
    "\n",
    "![Hyperopt](tpesearch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hpsklearn import HyperoptEstimator, gradient_boosting_regression\n",
    "from hyperopt import tpe\n",
    "\n",
    "estim = HyperoptEstimator(regressor=gradient_boosting_regression('reg'),  \n",
    "                            algo=tpe.suggest, max_evals=100)\n",
    "\n",
    "estim.fit(X_train, Y_train)\n",
    "\n",
    "print(estim.score(X_test, Y_test))\n",
    "print(estim.best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "\n",
    "model = Pipeline([('Scaler', estim.best_model()['preprocs'][0]),\n",
    "                 ('RF', estim.best_model()['learner'])])\n",
    "#model = estim.best_model()['learner']\n",
    "\n",
    "cross_val_score(model, X, Y, cv=3, scoring='r2').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя hyperopt, можно подбирать параметры у любых других моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание.\n",
    "Поработайте с датасетом для классификации https://archive.ics.uci.edu/ml/datasets/Alcohol+QCM+Sensor+Dataset#\n",
    "\n",
    "Загрузим все таблички из датасета (QCM.csv) и объединим их в одну таблицу. \n",
    "\n",
    "Последние 5 колонок содержат в себе целевую переменную. Создадим одну колонку с целевой переменной y следующим образом: если 1-Octanol = 1, то y = 0, если 1-Propanol = 1, то y=1, если 2-Butanol = 1, то y=2 и т.д.\n",
    "\n",
    "Создадим матрицу объект-признак X, содержащую все признаки (кроме последних 5 целевых колонок). \n",
    "\n",
    "1) Посчитайте качество DecisionTreeClassifier, RandomForestClassifier и GradientBoostingClassifier на кросс-валидации.\n",
    "\n",
    "2*) Попробуйте уменьшить число признаков с помощью какого-либо метода отбора признаков (http://scikit-learn.org/stable/modules/feature_selection.html). Добейтесь увеличения качества на кросс-валидации.\n",
    "\n",
    "3) Используйте gridsearch или hyperopt для подбора оптимальных параметров у леса и у бустинга. Какого наилучшего качества на кросс-валидации удалось достичь?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv(\"QCM3.csv\",delimiter=';')\n",
    "data2 = pd.read_csv(\"QCM6.csv\",delimiter=';')\n",
    "data3 = pd.read_csv(\"QCM7.csv\",delimiter=';')\n",
    "data4 = pd.read_csv(\"QCM10.csv\",delimiter=';')\n",
    "data5 = pd.read_csv(\"QCM12.csv\",delimiter=';')\n",
    "\n",
    "#your code here\n",
    "df = pd.concat([data1, data2, data3, data4, data5])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(x):\n",
    "    if x['1-Octanol'] == 1:\n",
    "        return 0\n",
    "    if x['1-Propanol'] == 1:\n",
    "        return 1\n",
    "    if x['2-Butanol'] == 1:\n",
    "        return 2\n",
    "    if x['2-propanol'] == 1:\n",
    "        return 3\n",
    "    if x['1-isobutanol'] == 1:\n",
    "        return 4\n",
    "    return -1\n",
    "    \n",
    "df['Target'] = df.apply(get_target, axis=1)\n",
    "\n",
    "df.drop(['1-Octanol','1-Propanol','2-Butanol','2-propanol','1-isobutanol'],\\\n",
    "        axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Target',axis=1)\n",
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "print(cross_val_score(dt, X, y, cv=5, scoring='accuracy').mean())\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for n in range(11, 21):\n",
    "    rf = RandomForestClassifier(n_estimators = n, max_depth=4)\n",
    "    \n",
    "    print('n = ',n)\n",
    "    print(cross_val_score(rf, X, y, cv=5, scoring='accuracy').mean())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
